{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/pietervanbrakel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/pietervanbrakel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/pietervanbrakel/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import string\n",
    "import pickle\n",
    "\n",
    "# Third-party libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Setup\n",
    "\n",
    "# tqdm setup\n",
    "tqdm.pandas()  # activeert progress_apply\n",
    "\n",
    "# NLTK downloads\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "# Constants\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing functions\n",
    "\n",
    "def lowercase_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert all characters in the input text to lowercase.\n",
    "    \n",
    "    Args:\n",
    "        text (str): A single string of text.\n",
    "\n",
    "    Returns:\n",
    "        str: Lowercased text.\n",
    "    \"\"\"\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "def remove_punctuation(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove all punctuation characters from the input text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): A single string of text.\n",
    "\n",
    "    Returns:\n",
    "        str: Text without punctuation.\n",
    "    \"\"\"\n",
    "    return \"\".join([c for c in text if c not in string.punctuation])\n",
    "\n",
    "\n",
    "def tokenize_text(text: str) -> list:\n",
    "    \"\"\"\n",
    "    Split text into individual word tokens.\n",
    "    \n",
    "    Args:\n",
    "        text (str): A single string of text.\n",
    "\n",
    "    Returns:\n",
    "        list: List of word tokens.\n",
    "    \"\"\"\n",
    "    return word_tokenize(text)\n",
    "\n",
    "\n",
    "def remove_stopwords(tokens: list) -> list:\n",
    "    \"\"\"\n",
    "    Remove English stopwords from a list of tokens.\n",
    "    \n",
    "    Args:\n",
    "        tokens (list): List of word tokens.\n",
    "\n",
    "    Returns:\n",
    "        list: List of tokens excluding stopwords.\n",
    "    \"\"\"\n",
    "    return [t for t in tokens if t not in stop_words]\n",
    "\n",
    "\n",
    "def lemmatize_tokens(tokens: list) -> list:\n",
    "    \"\"\"\n",
    "    Lemmatize each token to its base/dictionary form.\n",
    "    \n",
    "    Args:\n",
    "        tokens (list): List of word tokens.\n",
    "\n",
    "    Returns:\n",
    "        list: List of lemmatized tokens.\n",
    "    \"\"\"\n",
    "    return [lemmatizer.lemmatize(t) for t in tokens]\n",
    "\n",
    "\n",
    "def preprocess_text(text: str) -> list:\n",
    "    \"\"\"\n",
    "    Apply full preprocessing pipeline to text: lowercase, remove punctuation,\n",
    "    tokenize, remove stopwords, and lemmatize.\n",
    "    \n",
    "    Args:\n",
    "        text (str): A single string of text.\n",
    "\n",
    "    Returns:\n",
    "        list: Preprocessed list of tokens.\n",
    "    \"\"\"\n",
    "    text = lowercase_text(text)\n",
    "    text = remove_punctuation(text)\n",
    "    tokens = tokenize_text(text)\n",
    "    tokens = remove_stopwords(tokens)\n",
    "    tokens = lemmatize_tokens(tokens)\n",
    "    return tokens\n",
    "\n",
    "# Function to save data\n",
    "\n",
    "def save_pickle(obj, filepath: str):\n",
    "    \"\"\"\n",
    "    Save a Python object to a pickle file.\n",
    "    \n",
    "    Args:\n",
    "        obj: Python object to save (e.g., list, DataFrame, Series).\n",
    "        filepath (str): Path to the output pickle file.\n",
    "    \"\"\"\n",
    "    with open(filepath, \"wb\") as f:\n",
    "        pickle.dump(obj, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurable data path\n",
    "\n",
    "DATA_RAW = \"../data/raw\"  # <-- change this path if needed\n",
    "DATA_FILE = \"WELFake_Dataset.csv\"\n",
    "DATA_PATH = os.path.join(DATA_RAW, DATA_FILE)\n",
    "\n",
    "DATA_PROCESSED = \"../data/processed\"  # <-- change this folder if needed\n",
    "os.makedirs(DATA_PROCESSED, exist_ok=True)\n",
    "X_TRAIN_FILE = os.path.join(DATA_PROCESSED, \"X_train.pkl\")\n",
    "X_TEST_FILE = os.path.join(DATA_PROCESSED, \"X_test.pkl\")\n",
    "Y_TRAIN_FILE = os.path.join(DATA_PROCESSED, \"y_train.pkl\")\n",
    "Y_TEST_FILE = os.path.join(DATA_PROCESSED, \"y_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57229/57229 [02:04<00:00, 459.76it/s]\n",
      "100%|██████████| 14308/14308 [00:31<00:00, 454.87it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv(DATA_PATH, index_col=0)\n",
    "df = df.dropna()\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['text'], df['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# 7️⃣ Preprocessing\n",
    "X_train_tokens = X_train.progress_apply(preprocess_text)\n",
    "X_test_tokens = X_test.progress_apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed data saved in ../data/processed\n"
     ]
    }
   ],
   "source": [
    "# Save all datasets\n",
    "\n",
    "save_pickle(X_train_tokens, X_TRAIN_FILE)\n",
    "save_pickle(X_test_tokens, X_TEST_FILE)\n",
    "save_pickle(y_train, Y_TRAIN_FILE)\n",
    "save_pickle(y_test, Y_TEST_FILE)\n",
    "\n",
    "print(f\"Preprocessed data saved in {DATA_PROCESSED}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
